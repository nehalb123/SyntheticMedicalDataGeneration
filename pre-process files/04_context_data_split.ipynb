{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be generating different permutations of the context data for the source files we created in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_data_split(filename, dataset_split):\n",
    "\n",
    "    directory = \"../data/preprocessed/other_contexts/\"\n",
    "    \n",
    "    f_h = open(directory + \"src-\" + dataset_split + \"-h.txt\",\"w+\")\n",
    "    f_h_gae = open(directory + \"src-\" + dataset_split + \"-h-gae.txt\",\"w+\")\n",
    "    f_h_gae_d = open(directory + \"src-\" + dataset_split + \"-h-gae-d.txt\",\"w+\")\n",
    "    f_h_gae_p = open(directory + \"src-\" + dataset_split + \"-h-gae-p.txt\",\"w+\")\n",
    "    f_h_gae_d_p = open(directory + \"src-\" + dataset_split + \"-h-gae-d-p.txt\",\"w+\")\n",
    "    f_h_gae_d_p_m = open(directory + \"src-\" + dataset_split + \"-h-gae-d-p-m.txt\",\"w+\")\n",
    "    f_h_gae_d_p_m_t = open(directory + \"src-\" + dataset_split + \"-h-gae-d-p-m-t.txt\",\"w+\")\n",
    "    f_h_gae_d_p_m_l = open(directory + \"src-\" + dataset_split + \"-h-gae-d-p-m-l.txt\",\"w+\")\n",
    "    \n",
    "    f = open(filename,'r')\n",
    "    line = f.readline()\n",
    "\n",
    "    while line:\n",
    "        context = re.split(r' <H> | <G> | <A> | <E> | <D> | <P> | <M> | <T> | <L>', line)\n",
    "\n",
    "        hint = context[0] + \" <H> \"\n",
    "        demographic = context[1] + \" <G> \" + context[2] + \" <A> \" + context[3] + \" <E> \"\n",
    "        diagnosis_list = context[4] + \" <D> \"\n",
    "        procedure_list = context[5] + \" <P> \"\n",
    "        med_list = context[6] + \" <M> \"\n",
    "        microbio_list = context[7] + \" <T> \"\n",
    "        lab_list = context[8] + \" <L>\"\n",
    "\n",
    "        H = hint + \"\\n\"\n",
    "        H_GAE = hint + demographic + \"\\n\"\n",
    "        H_GAE_D = hint + demographic + diagnosis_list + \"\\n\"\n",
    "        H_GAE_P = hint + demographic + diagnosis_list + procedure_list + \"\\n\"\n",
    "        H_GAE_D_P = hint + demographic + diagnosis_list + procedure_list + \"\\n\"\n",
    "        H_GAE_D_P_M = hint + demographic + diagnosis_list + procedure_list + med_list + \"\\n\"\n",
    "        H_GAE_D_P_M_T = hint + demographic + diagnosis_list + procedure_list + med_list + microbio_list + \"\\n\"\n",
    "        H_GAE_D_P_M_L = hint + demographic + diagnosis_list + procedure_list + med_list + lab_list + \"\\n\"\n",
    "\n",
    "        f_h.write(H)\n",
    "        f_h_gae.write(H_GAE)\n",
    "        f_h_gae_d.write(H_GAE_D)\n",
    "        f_h_gae_p.write(H_GAE_P)\n",
    "        f_h_gae_d_p.write(H_GAE_D_P)\n",
    "        f_h_gae_d_p_m.write(H_GAE_D_P_M)\n",
    "        f_h_gae_d_p_m_t.write(H_GAE_D_P_M_T)\n",
    "        f_h_gae_d_p_m_l.write(H_GAE_D_P_M_L)\n",
    "\n",
    "        line = f.readline()\n",
    "    \n",
    "    f_h.close()\n",
    "    f_h_gae.close()\n",
    "    f_h_gae_d.close()\n",
    "    f_h_gae_p.close()\n",
    "    f_h_gae_d_p.close()\n",
    "    f_h_gae_d_p_m.close()\n",
    "    f_h_gae_d_p_m_t.close()\n",
    "    f_h_gae_d_p_m_l.close()\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting training data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/preprocessed/other_contexts/src-train-h.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplitting training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcontext_data_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/preprocessed/src-train.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplitting evaluation data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m context_data_split(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/preprocessed/src-val.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [2], line 5\u001b[0m, in \u001b[0;36mcontext_data_split\u001b[0;34m(filename, dataset_split)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontext_data_split\u001b[39m(filename, dataset_split):\n\u001b[1;32m      3\u001b[0m     directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/preprocessed/other_contexts/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     f_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msrc-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-h.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     f_h_gae \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_split \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-h-gae.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     f_h_gae_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dataset_split \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-h-gae-d.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/preprocessed/other_contexts/src-train-h.txt'"
     ]
    }
   ],
   "source": [
    "print (\"splitting training data...\")\n",
    "context_data_split(\"../data/preprocessed/src-train.txt\",\"train\")\n",
    "\n",
    "print (\"splitting evaluation data...\")\n",
    "context_data_split(\"../data/preprocessed/src-val.txt\",\"val\")\n",
    "\n",
    "print (\"splitting test data...\")\n",
    "context_data_split(\"../data/preprocessed/src-test.txt\",\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that all files have the same number of lines and that the word/character counts are in line with expectation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mimic/data/preprocessed/other_contexts\n",
      "     5727   2571310  12500237 src-test-h-gae-d-p-m-l.txt\n",
      "     5727    878941   5527487 src-test-h-gae-d-p-m-t.txt\n",
      "     5727    841031   5339036 src-test-h-gae-d-p-m.txt\n",
      "     5727    613073   4146137 src-test-h-gae-d-p.txt\n",
      "     5727    553588   3724383 src-test-h-gae-d.txt\n",
      "     5727    613073   4146137 src-test-h-gae-p.txt\n",
      "     5727     97359    442100 src-test-h-gae.txt\n",
      "     5727     62997    309842 src-test-h.txt\n",
      "    44230  19997807  97595528 src-train-h-gae-d-p-m-l.txt\n",
      "    44230   6958046  43839444 src-train-h-gae-d-p-m-t.txt\n",
      "    44230   6689483  42521849 src-train-h-gae-d-p-m.txt\n",
      "    44230   4827069  32745113 src-train-h-gae-d-p.txt\n",
      "    44230   3920392  26260356 src-train-h-gae-d.txt\n",
      "    44230   4827069  32745113 src-train-h-gae-p.txt\n",
      "    44230    751910   3427563 src-train-h-gae.txt\n",
      "    44230    486530   2405680 src-train-h.txt\n",
      "     5447   2468944  12034600 src-val-h-gae-d-p-m-l.txt\n",
      "     5447    851598   5367720 src-val-h-gae-d-p-m-t.txt\n",
      "     5447    817634   5200961 src-val-h-gae-d-p-m.txt\n",
      "     5447    590960   4008708 src-val-h-gae-d-p.txt\n",
      "     5447    480552   3218051 src-val-h-gae-d.txt\n",
      "     5447    590960   4008708 src-val-h-gae-p.txt\n",
      "     5447     92599    422324 src-val-h-gae.txt\n",
      "     5447     59917    296473 src-val-h.txt\n",
      "   443232  60642842 352233550 total\n"
     ]
    }
   ],
   "source": [
    "%cd ../data/preprocessed/other_contexts/\n",
    "!wc -mlw *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
