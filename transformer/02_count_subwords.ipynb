{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor2tensor uses subwords as tokens by default which results in better performance. It also uses steps fo determining the length of training as opposed to epochs. Converting between number of steps and number of epochs is based on the batch effective size (i.e. `effective_batch_size = batch_size * num_of_gpus`) and the number of subwords in a batch such that `epochs = steps * effective_batch_size / training_subwords`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, rather unhelpfully, t2t does not provide us with the number of subwords in our dataset or in an individual sample so in order to convert between steps and epochs, this must be done manually using the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor2tensor.data_generators import text_encoder\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0904 18:32:46.797372 139836362250048 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/text_encoder.py:938: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "W0904 18:32:46.798849 139836362250048 deprecation_wrapper.py:119] From /home/aa5118/anaconda3/envs/tf/lib/python3.7/site-packages/tensor2tensor/data_generators/text_encoder.py:940: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_filepath = '../data/t2t_experiments/transformer/low_resource/full_context/data/vocab.mimic_discharge_summaries.32768.subwords'\n",
    "vocab = text_encoder.SubwordTextEncoder(vocab_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sp.getoutput('head -1 ../data/preprocessed/src-train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n"
     ]
    }
   ],
   "source": [
    "print(len([vocab._subtoken_ids_to_tokens([x]) for x in vocab.encode(text)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "509 subwords in that example - the final input context in our training set. Let's use this method to work out the total number of subwords in our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,357,469\n",
      "1,726,779\n",
      "2,196,080\n"
     ]
    }
   ],
   "source": [
    "total_subword_count = 0\n",
    "max_512_subword_count = 0\n",
    "max_1024_subword_count = 0\n",
    "\n",
    "f=open(\"../data/preprocessed/low_resource/src-train.txt\",\"r\")\n",
    "\n",
    "line = f.readline()\n",
    "\n",
    "while line:\n",
    "    \n",
    "    count = len([vocab._subtoken_ids_to_tokens([x]) for x in vocab.encode(line)])\n",
    "    total_subword_count += count\n",
    "    max_512_subword_count += min(512,count)\n",
    "    max_1024_subword_count += min(1024,count)\n",
    "    line = f.readline()\n",
    "\n",
    "f.close()\n",
    "\n",
    "print ('{:,}'.format(total_subword_count))\n",
    "print ('{:,}'.format(max_512_subword_count))\n",
    "print ('{:,}'.format(max_1024_subword_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~25.7m subwords in source training file consisting ~40k discharge summaries. We can now use this number to calculate how many training steps corresponds to one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_gpus = 4\n",
    "batch_size = 4096\n",
    "effective_batch_size = num_of_gpus * batch_size\n",
    "epochs = 1\n",
    "\n",
    "def epoch2steps(subword_count):\n",
    "    steps = epochs / (effective_batch_size/subword_count)\n",
    "    print (\"1 epoch correponds to\", '{:,}'.format(int(steps)), \"steps\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch correponds to 143 steps\n",
      "1 epoch correponds to 105 steps\n",
      "1 epoch correponds to 134 steps\n"
     ]
    }
   ],
   "source": [
    "epoch2steps(total_subword_count)\n",
    "epoch2steps(max_512_subword_count)\n",
    "epoch2steps(max_1024_subword_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liu et al (2018) use 400,000 steps when training their transformer model. With this setup, this would correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 epochs\n"
     ]
    }
   ],
   "source": [
    "print('{0:,}'.format(round(400000 / steps)), \"epochs\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not a valid comparison because we are only focusing on discharge summaries whereas Liu at el where looking at the entire dataset of notes of which discharge summaries are only a small percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important consideration point is the fact that Liu et al (2018) limit/truncate both input and output tokens (subwords) to 512. As shown above, this ends up removing ~7m subwords from our input. Doubling this to 1024 is worth considering as this only removes ~2m words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
